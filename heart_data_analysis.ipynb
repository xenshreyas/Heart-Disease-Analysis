{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6183be2-fd42-4281-ada1-44774ec92c45",
   "metadata": {},
   "source": [
    "## **Can <font color='darkred'>Heart Disease</font> be predicted?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512944b-aa62-4ce1-b294-9d2f2b9cd802",
   "metadata": {},
   "source": [
    "**Authors:** Lisa Desjarlais, Shreyas Goyal, and Emma Szeto _(Group 155)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a5575-5295-4219-bbd6-8f35b771a4b0",
   "metadata": {},
   "source": [
    "<center><img src=\"https://gifs.eco.br/wp-content/uploads/2022/08/gifs-do-sistema-cardiovascular-0.gif\" alt=\"Pictorial representation of a Human Heart\" width=\"40%\"> <img src=\"https://lh5.googleusercontent.com/hxLpYW3mP53JJSnWvUL9bB8zWvezzLWVHklVYPm2owl3xyik3PtPibwK5lXMy98fKICJZK7Ee8EP7ZvIUTVCey4ZW0_SftMKeMeooft9MoRiYOTdO2S2duo6QgiArcqZ9Y8qfe-E\" alt=\"Heart Disease in USA\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d23d0f3-8070-4957-9af2-e58a3a97c52c",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "    \n",
    "According to medical professionals at Johns Hopkins and Mount Sinai Health, there are five classifications of heart disease: Class 0 indicates no presence of heart disease, while classes 1 to 4 describe mild, moderate, severe and very severe heart disease, in that order <a href=\"(https://www.hopkinsmedicine.org/news/media/releases/updated_classification_system_captures_many_more_people_at_risk_for_heart_attack\">(Johns Hopkins Medicine 2017)</a>. The goal of our project is to answer the predictive question: Can we use the health attributes available to us to predict whether a future patient (with an unknown diagnosis) has heart disease? For our project we will be analysing both the <a href=\"https://archive.ics.uci.edu/ml/datasets/Heart+Disease\">Cleveland and Hungarian Heart Disease Data Sets.</a>  The Cleveland data set was compiled using the medical information of 303 patients, while the Hungary data set used 294 patients. Each row in the data sets represents a patient with some classification of heart disease and 13 other health attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d9762-e560-4940-9898-8ac61c96c0a9",
   "metadata": {},
   "source": [
    "**Preprocessing the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed38e59-162d-4f03-96bb-179e00cf50de",
   "metadata": {},
   "source": [
    "We begin our analysis by loading in all necessary libraries and our two data sets. We are using the tidyverse library to load, wrangle and plot our data. We set the seed as 14 such that our work is reproducible. We then proceed to wrangle, clean, add in our binary classification column, as was done in the proposal. Then, we combine our two data sets into one heart data set, which now has the medical information of 597 patients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f20b8a-e88c-4ef9-a1dc-e614e0cfa01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.6     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.7     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.9\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.2     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "── \u001b[1mAttaching packages\u001b[22m ────────────────────────────────────── tidymodels 1.0.0 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mbroom       \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mrsample     \u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdials       \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mtune        \u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34minfer       \u001b[39m 1.0.2     \u001b[32m✔\u001b[39m \u001b[34mworkflows   \u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mmodeldata   \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mworkflowsets\u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mparsnip     \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34myardstick   \u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mrecipes     \u001b[39m 1.0.1     \n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ───────────────────────────────────────── tidymodels_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mscales\u001b[39m::\u001b[32mdiscard()\u001b[39m masks \u001b[34mpurrr\u001b[39m::discard()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m   masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mfixed()\u001b[39m  masks \u001b[34mstringr\u001b[39m::fixed()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m      masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34myardstick\u001b[39m::\u001b[32mspec()\u001b[39m masks \u001b[34mreadr\u001b[39m::spec()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m   masks \u001b[34mstats\u001b[39m::step()\n",
      "\u001b[34m•\u001b[39m Learn how to get started at \u001b[32mhttps://www.tidymodels.org/start/\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading in all necessary libraries.\n",
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "options(repr.matrix.max.rows = 10)\n",
    "\n",
    "# Setting the seed.\n",
    "set.seed(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "654e3d1a-ac3e-4f78-b011-8a03fc03e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in mask$eval_all_mutate(quo):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in mask$eval_all_mutate(quo):\n",
      "“NAs introduced by coercion”\n"
     ]
    }
   ],
   "source": [
    "# Loading in Cleveland data set\n",
    "URL <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "\n",
    "cleveland_data <- read_csv(URL, col_names=FALSE, show_col_types = FALSE)\n",
    "\n",
    "# Since column names aren't pre-specified, we rename them using colnames, according to the UCI repository.\n",
    "colnames(cleveland_data) <- c(\"age\", \"sex\", \"cp\", \"trestbps\", \n",
    "                              \"chol\", \"fbs\", \"restecg\", \"thalach\", \n",
    "                              \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\")\n",
    "\n",
    "cleveland_data <- cleveland_data |>\n",
    "    mutate(ca = as.numeric(ca), thal = as.numeric(thal)) |>\n",
    "    filter(ca != \"NA\", thal != \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e394317b-d2ff-4b9d-bfc5-099d8ca9383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding column indicating yes or no for presence of heart disease\n",
    "cl_have_disease <- cleveland_data |>\n",
    "    filter(num == 1 | num == 2 | num == 3 | num == 4) |>\n",
    "    mutate(hd = \"yes\") |>\n",
    "    mutate(hd = as_factor(hd))\n",
    "cl_no_disease <- cleveland_data |>\n",
    "    filter(num == 0) |>\n",
    "    mutate(hd = \"no\") |>\n",
    "    mutate(hd = as_factor(hd))\n",
    "cl_data_alt <- rbind(cl_have_disease, cl_no_disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df1733b-a897-48c3-8eba-3dafd8172357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in Hungarian data set\n",
    "URL2 <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/reprocessed.hungarian.data\"\n",
    "\n",
    "hungarian_data <- read_delim(URL2, col_names = FALSE, delim = \" \", show_col_types = FALSE)\n",
    "\n",
    "colnames(hungarian_data) <- c(\"age\", \"sex\", \"cp\", \"trestbps\", \n",
    "                              \"chol\", \"fbs\", \"restecg\", \"thalach\", \n",
    "                              \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\")\n",
    "\n",
    "\n",
    "# Combine levels 1,2,3,4 into hd column. Levels 1-4 as \"yes\", level 0 as \"no.\"\n",
    "hung_have_disease <- hungarian_data |>\n",
    "    filter(num == 1 | num == 2 | num == 3 | num == 4) |>\n",
    "    mutate(hd = \"yes\") |>\n",
    "    mutate(hd = as_factor(hd))\n",
    "\n",
    "hung_no_disease <- hungarian_data |>\n",
    "    filter(num == 0) |>\n",
    "    mutate(hd = \"no\") |>\n",
    "    mutate(hd = as_factor(hd))\n",
    "\n",
    "hung_data_alt <- rbind(hung_have_disease, hung_no_disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8720e93c-cc13-4c3b-8084-0767e6e83b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 591 × 14</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>age</th><th scope=col>sex</th><th scope=col>cp</th><th scope=col>trestbps</th><th scope=col>chol</th><th scope=col>fbs</th><th scope=col>restecg</th><th scope=col>thalach</th><th scope=col>exang</th><th scope=col>oldpeak</th><th scope=col>slope</th><th scope=col>ca</th><th scope=col>thal</th><th scope=col>hd</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>49</td><td>0</td><td>3</td><td>160</td><td>180</td><td>0</td><td>0</td><td>156</td><td>0</td><td>1.0</td><td>2</td><td>-9</td><td>-9</td><td>yes</td></tr>\n",
       "\t<tr><td>48</td><td>0</td><td>4</td><td>138</td><td>214</td><td>0</td><td>0</td><td>108</td><td>1</td><td>1.5</td><td>2</td><td>-9</td><td>-9</td><td>yes</td></tr>\n",
       "\t<tr><td>37</td><td>1</td><td>4</td><td>140</td><td>207</td><td>0</td><td>0</td><td>130</td><td>1</td><td>1.5</td><td>2</td><td>-9</td><td>-9</td><td>yes</td></tr>\n",
       "\t<tr><td>58</td><td>1</td><td>2</td><td>136</td><td>164</td><td>0</td><td>1</td><td> 99</td><td>1</td><td>2.0</td><td>2</td><td>-9</td><td>-9</td><td>yes</td></tr>\n",
       "\t<tr><td>49</td><td>1</td><td>4</td><td>140</td><td>234</td><td>0</td><td>0</td><td>140</td><td>1</td><td>1.0</td><td>2</td><td>-9</td><td>-9</td><td>yes</td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>35</td><td>1</td><td>2</td><td>122</td><td>192</td><td>0</td><td>0</td><td>174</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>3</td><td>no</td></tr>\n",
       "\t<tr><td>56</td><td>1</td><td>2</td><td>130</td><td>221</td><td>0</td><td>2</td><td>163</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>7</td><td>no</td></tr>\n",
       "\t<tr><td>56</td><td>1</td><td>2</td><td>120</td><td>240</td><td>0</td><td>0</td><td>169</td><td>0</td><td>0.0</td><td>3</td><td>0</td><td>3</td><td>no</td></tr>\n",
       "\t<tr><td>55</td><td>0</td><td>2</td><td>132</td><td>342</td><td>0</td><td>0</td><td>166</td><td>0</td><td>1.2</td><td>1</td><td>0</td><td>3</td><td>no</td></tr>\n",
       "\t<tr><td>41</td><td>1</td><td>2</td><td>120</td><td>157</td><td>0</td><td>0</td><td>182</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>3</td><td>no</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 591 × 14\n",
       "\\begin{tabular}{llllllllllllll}\n",
       " age & sex & cp & trestbps & chol & fbs & restecg & thalach & exang & oldpeak & slope & ca & thal & hd\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t 49 & 0 & 3 & 160 & 180 & 0 & 0 & 156 & 0 & 1.0 & 2 & -9 & -9 & yes\\\\\n",
       "\t 48 & 0 & 4 & 138 & 214 & 0 & 0 & 108 & 1 & 1.5 & 2 & -9 & -9 & yes\\\\\n",
       "\t 37 & 1 & 4 & 140 & 207 & 0 & 0 & 130 & 1 & 1.5 & 2 & -9 & -9 & yes\\\\\n",
       "\t 58 & 1 & 2 & 136 & 164 & 0 & 1 &  99 & 1 & 2.0 & 2 & -9 & -9 & yes\\\\\n",
       "\t 49 & 1 & 4 & 140 & 234 & 0 & 0 & 140 & 1 & 1.0 & 2 & -9 & -9 & yes\\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 35 & 1 & 2 & 122 & 192 & 0 & 0 & 174 & 0 & 0.0 & 1 & 0 & 3 & no\\\\\n",
       "\t 56 & 1 & 2 & 130 & 221 & 0 & 2 & 163 & 0 & 0.0 & 1 & 0 & 7 & no\\\\\n",
       "\t 56 & 1 & 2 & 120 & 240 & 0 & 0 & 169 & 0 & 0.0 & 3 & 0 & 3 & no\\\\\n",
       "\t 55 & 0 & 2 & 132 & 342 & 0 & 0 & 166 & 0 & 1.2 & 1 & 0 & 3 & no\\\\\n",
       "\t 41 & 1 & 2 & 120 & 157 & 0 & 0 & 182 & 0 & 0.0 & 1 & 0 & 3 & no\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 591 × 14\n",
       "\n",
       "| age &lt;dbl&gt; | sex &lt;dbl&gt; | cp &lt;dbl&gt; | trestbps &lt;dbl&gt; | chol &lt;dbl&gt; | fbs &lt;dbl&gt; | restecg &lt;dbl&gt; | thalach &lt;dbl&gt; | exang &lt;dbl&gt; | oldpeak &lt;dbl&gt; | slope &lt;dbl&gt; | ca &lt;dbl&gt; | thal &lt;dbl&gt; | hd &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 49 | 0 | 3 | 160 | 180 | 0 | 0 | 156 | 0 | 1.0 | 2 | -9 | -9 | yes |\n",
       "| 48 | 0 | 4 | 138 | 214 | 0 | 0 | 108 | 1 | 1.5 | 2 | -9 | -9 | yes |\n",
       "| 37 | 1 | 4 | 140 | 207 | 0 | 0 | 130 | 1 | 1.5 | 2 | -9 | -9 | yes |\n",
       "| 58 | 1 | 2 | 136 | 164 | 0 | 1 |  99 | 1 | 2.0 | 2 | -9 | -9 | yes |\n",
       "| 49 | 1 | 4 | 140 | 234 | 0 | 0 | 140 | 1 | 1.0 | 2 | -9 | -9 | yes |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 35 | 1 | 2 | 122 | 192 | 0 | 0 | 174 | 0 | 0.0 | 1 | 0 | 3 | no |\n",
       "| 56 | 1 | 2 | 130 | 221 | 0 | 2 | 163 | 0 | 0.0 | 1 | 0 | 7 | no |\n",
       "| 56 | 1 | 2 | 120 | 240 | 0 | 0 | 169 | 0 | 0.0 | 3 | 0 | 3 | no |\n",
       "| 55 | 0 | 2 | 132 | 342 | 0 | 0 | 166 | 0 | 1.2 | 1 | 0 | 3 | no |\n",
       "| 41 | 1 | 2 | 120 | 157 | 0 | 0 | 182 | 0 | 0.0 | 1 | 0 | 3 | no |\n",
       "\n"
      ],
      "text/plain": [
       "    age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal\n",
       "1   49  0   3  160      180  0   0       156     0     1.0     2     -9 -9  \n",
       "2   48  0   4  138      214  0   0       108     1     1.5     2     -9 -9  \n",
       "3   37  1   4  140      207  0   0       130     1     1.5     2     -9 -9  \n",
       "4   58  1   2  136      164  0   1        99     1     2.0     2     -9 -9  \n",
       "5   49  1   4  140      234  0   0       140     1     1.0     2     -9 -9  \n",
       "⋮   ⋮   ⋮   ⋮  ⋮        ⋮    ⋮   ⋮       ⋮       ⋮     ⋮       ⋮     ⋮  ⋮   \n",
       "587 35  1   2  122      192  0   0       174     0     0.0     1     0  3   \n",
       "588 56  1   2  130      221  0   2       163     0     0.0     1     0  7   \n",
       "589 56  1   2  120      240  0   0       169     0     0.0     3     0  3   \n",
       "590 55  0   2  132      342  0   0       166     0     1.2     1     0  3   \n",
       "591 41  1   2  120      157  0   0       182     0     0.0     1     0  3   \n",
       "    hd \n",
       "1   yes\n",
       "2   yes\n",
       "3   yes\n",
       "4   yes\n",
       "5   yes\n",
       "⋮   ⋮  \n",
       "587 no \n",
       "588 no \n",
       "589 no \n",
       "590 no \n",
       "591 no "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heart_data <- rbind(hung_data_alt, cl_data_alt) |>\n",
    "    select(-num)\n",
    "heart_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939db0cc-dc1f-47d3-b15c-f46d2a17bedb",
   "metadata": {},
   "source": [
    "Table 1. _The recorded medical data of all patients, alongside whether or not they have heart disease._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99ee40-b244-411a-894d-7ac9ca2d5687",
   "metadata": {},
   "source": [
    "We then make note of the number of observations in our two classes. This helps us gauge how well our classifier is doing, since we can compare it to the majority classifier. If our classifier can improve its accuracy over 59% (our majority percentage), then our method is gaining valuable information from our predictors <a href=\"https://datasciencebook.ca/classification2.html#critically-analyze-performance\">(Timbers et al., 2022)</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0218884-32bd-442d-8ace-5184dc89a8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 2 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>hd</th><th scope=col>count</th><th scope=col>percentage</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>yes</td><td>243</td><td>41.11675</td></tr>\n",
       "\t<tr><td>no </td><td>348</td><td>58.88325</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 2 × 3\n",
       "\\begin{tabular}{lll}\n",
       " hd & count & percentage\\\\\n",
       " <fct> & <int> & <dbl>\\\\\n",
       "\\hline\n",
       "\t yes & 243 & 41.11675\\\\\n",
       "\t no  & 348 & 58.88325\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 2 × 3\n",
       "\n",
       "| hd &lt;fct&gt; | count &lt;int&gt; | percentage &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| yes | 243 | 41.11675 |\n",
       "| no  | 348 | 58.88325 |\n",
       "\n"
      ],
      "text/plain": [
       "  hd  count percentage\n",
       "1 yes 243   41.11675  \n",
       "2 no  348   58.88325  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Percentages of hd (yes, no) \n",
    "num_obs_h <- nrow(heart_data)\n",
    "heart_proportions <- heart_data |>\n",
    "    group_by(hd) |>\n",
    "    summarize(\n",
    "        count = n(),\n",
    "        percentage = n() / num_obs_h * 100\n",
    "        )\n",
    "heart_proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e07b41-e40b-4ba8-931c-c07168d7cc47",
   "metadata": {},
   "source": [
    "Table 2. _The amount of patients with and without Heart Disease._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843452c-7a80-472f-8daf-9d835f531054",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947649a3-7212-4896-bfe7-d05fe813e55f",
   "metadata": {},
   "source": [
    "**Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a829f0e-67f0-4849-bc59-5070c3ea06aa",
   "metadata": {},
   "source": [
    "We now begin our K-nn classification by splitting our heart data set into a training and testing set. 75% of our data will be allocated to the training set and 25% for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6c8f2a-4ffd-4b1e-ab79-e58ad5ddd19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Training and Testing Data Sets\n",
    "heart_split <- initial_split(heart_data, prop = 0.75, strata = hd)\n",
    "heart_training <- training(heart_split)\n",
    "heart_testing <- testing(heart_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a394c-513b-4be6-a1f2-e50bfd3d5623",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d231e117-73ae-4632-9f49-1e0fb37f68d6",
   "metadata": {},
   "source": [
    "**Finding the Best Predictors using Forward Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343a6d8-22cc-4a7a-8ea5-3210d4136d4c",
   "metadata": {},
   "source": [
    "In order to construct our classifier, we first need to select suitable predictors that can help us classify a new observation . Since we have $13$ variables to work with, we chose the method of forward selection to select the best predictors. According to the textbook, best subset selection is a method that trains all combinations of predictors and chooses the best accuracy. However, the method can become very slow once the number of predictors reaches 10 <a href=\"https://datasciencebook.ca/classification2.html#predictor-variable-selection\">(Timbers et al., 2022)</a>. With $m = 13$ variables, we would have to train $2^m-1=8191$ models. Therefore, we chose to implement the method of forward selection, which chooses a list of candidate models and is more efficient to run (since it only trains $1/2m(m+1)=91$ models). The only tradeoff is perhaps a reduction in possible accuracy since not all possible models are trained  <a href=\"https://datasciencebook.ca/classification2.html#predictor-variable-selection\">(Timbers et al., 2022)</a>. We felt this method fit the requirements of our project. The code for the forward selection method is from the textbook, section 6.8.3 <a href=\"https://datasciencebook.ca/classification2.html#predictor-variable-selection\">(Timbers et al., 2022)</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af155fa9-a2f9-48f5-afd4-fd1dc11d207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Forward Selection Code to find best predictors\n",
    "names <- colnames(heart_training |> select(-hd))\n",
    "\n",
    "# Create a tibble to store accuracies\n",
    "preds_accuracies <- tibble(size = integer(), \n",
    "                     model_string = character(), \n",
    "                     accuracy = numeric())\n",
    "\n",
    "# Create a model spec\n",
    "knn_FS_spec <- nearest_neighbor(weight_func = \"rectangular\",\n",
    "                             neighbors = tune()) |>\n",
    "    set_engine(\"kknn\") |>\n",
    "    set_mode(\"classification\")\n",
    "\n",
    "# Create a 5-fold cross-validation object\n",
    "heart_vfold_preds <- vfold_cv(heart_training, v = 5, strata = hd)\n",
    "\n",
    "# Stores selected predictors\n",
    "selected <- c()\n",
    "# For every size from 1 to 13 predictors\n",
    "for (i in 1:13) {\n",
    "    # Stores preds not yet added\n",
    "    accs <- list()\n",
    "    models <- list()\n",
    "    for (j in 1:length(names)) {\n",
    "        # For each combo of preds we are creating a model string\n",
    "        # and tuning the resultant classifier while collecting the accuracy\n",
    "        # of the best k\n",
    "        preds_new <- c(selected, names[[j]])\n",
    "        model_string <- paste(\"hd\", \"~\", paste(preds_new, collapse=\"+\"))\n",
    "        heart_FS_recipe <- recipe(as.formula(model_string),\n",
    "                                data = heart_training) |>\n",
    "                            step_scale(all_predictors()) |>\n",
    "                            step_center(all_predictors())\n",
    "        acc <- workflow() |>\n",
    "            add_recipe(heart_FS_recipe) |>\n",
    "            add_model(knn_FS_spec) |>\n",
    "            tune_grid(resamples = heart_vfold_preds, grid = 10) |>\n",
    "            collect_metrics() |>\n",
    "            filter(.metric == \"accuracy\") |>\n",
    "            summarize(mx = max(mean))\n",
    "        acc <- acc$mx |> unlist()\n",
    "        # Adding the result to the dataframe\n",
    "        accs[[j]] <- acc\n",
    "        models[[j]] <- model_string\n",
    "    }\n",
    "    jstar <- which.max(unlist(accs))\n",
    "    preds_accuracies <- preds_accuracies |>\n",
    "        add_row(size = i,\n",
    "                model_string = models[[jstar]],\n",
    "                accuracy = accs[[jstar]])\n",
    "    selected <- c(selected, names[[jstar]])\n",
    "    names <- names[-jstar]\n",
    "}\n",
    "preds_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd3106-5363-4ef9-bf34-07e74cb64954",
   "metadata": {},
   "source": [
    "Table 3. _The estimated accuracy of classifier using different combinations of predictors._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615aa344-0d8d-42eb-a076-2213acfcd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_preds_plot <- ggplot(preds_accuracies, aes(x = size, y = accuracy)) +\n",
    "    geom_line() +\n",
    "    xlab(\"Number of Predictors\") +\n",
    "    ylab(\"Estimated Accuracy\") +\n",
    "    scale_x_continuous(breaks = 1:13) +\n",
    "    ggtitle(\"Estimated Accuracy vs Number of Predictiors\") +\n",
    "    theme(text = element_text(size = 15))\n",
    "\n",
    "best_preds_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce1681-164d-4802-8796-6d19fe19f046",
   "metadata": {},
   "source": [
    "Figure 1. _Graph of Estimated Accuracy vs. Number of Predictors._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c390196b-63f3-47dc-8fc6-371e200496b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_preds <- preds_accuracies |> slice(3,4,12)\n",
    "best_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8b1af-0fe7-437f-8427-0c3b0d92f73d",
   "metadata": {},
   "source": [
    "Table 4. _The best 3 predicted accuracies, i.e. for 3, 4 and 12 predictors._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b62be0-e5b3-4c21-98f7-d0a97ba15849",
   "metadata": {},
   "source": [
    "The best accuracy comes from using 12 predictors. However this may cause an overfitting of data, given that it involves a lot of predictors and it comes after some oscillation of accuracy in the data. <a href=\"https://pubs.acs.org/doi/10.1021/ci0342472\">Hawkins (2004)</a> warns that overfitting is likely to occur when a more complex model is chosen over a simpler model with similar accuracy. According to  <a href=\"https://www.sciencedirect.com/science/article/pii/S0924271617303088\">A.D. Rocha et al. (2017)</a>, overfitting may produce a high accuracy on the training data, but a lower accuracy on the test data. They also describe how a predictor may be irrelevant in predicting the class variable but may correlate well with another variable relevant to the class variable, resulting in a false accuracy when both variables are used. So choosing 12 predictors might not be the best option. The next best accuracy comes from using just 3 predictors, which is a lot simpler. Thus, the predictors we will choose to train our classifier are \"oldpeak\", \"cp\", and \"thal\". These columns represent the following medical information:\n",
    "\n",
    "oldpeak: ST depression induced by exercise relative to rest\n",
    "\n",
    "cp: chest pain type (1 = typical angina, 2 = atypical angina, 3 = non-anginal pain, 4 = asymptomatic)\n",
    "\n",
    "thal: presence of heart defect (3 = normal, 6 = fixed defect, 7 = reversible defect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb6ebd-9de4-4a88-8226-cf9b0aea12dd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86586a-5ded-4faa-a59d-af9c7f9c2f5b",
   "metadata": {},
   "source": [
    "**Tuning the Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f22d80-de5a-489f-90a2-cf805f8376cb",
   "metadata": {},
   "source": [
    "Now, we want to tune the classifier in order to identify the best value of neighbours (k), in order to get the best accuracy for our classifier. We do this by tuning our classifier, which means testing different values of k, and then comparing the results at the end in order to choose that value of k for which our classifier's accuracy is the highest.\n",
    "\n",
    "While tuning the classifier, we can create multiple classifiers based on multiple splits of the training data itself, evaluate them and then choose a value for the neighbours we want based on all the different results. This is known as V-Fold Cross Validation. We do this in order to reduce the chances of our classifier being arbitrarily good or bad, based on whether our data was lucky enough to end up in the validation set. Here, we perform a 10-fold Cross Validation, thereby using 10 different splits of our training data, in order to determine the best value of k.\n",
    "\n",
    "Now, we first create a recipe that our classifier will follow, using the predictors that best predict heart disease, as found above. We step and scale all our predictors in order to make sure that our predictors have an equal influence on our classifier. Now, we can tune our classifier by using the nearest_neighbor() function, by setting the engine to \"kknn\" and mode to \"classification\" since we're performing a k-nn Classification. Now, we make a tibble of numbers from 1 to 50, which is a reasonable amount of neighbours to consider when we are looking at a population of 597 patients. Then, we combine everything into a workflow and produce the results of accuracies, for all the different possible values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645e17c-a90d-46fd-b788-06764f72e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 10-fold cross-validation object\n",
    "set.seed(1975)\n",
    "heart_vfold <- vfold_cv(heart_training, v = 10, strata = hd)\n",
    "\n",
    "# Preparing the recipe according to the best predictors as found above\n",
    "heart_recipe <- recipe(hd ~ oldpeak + cp + thal, data = heart_training) |>\n",
    "                  step_scale(all_predictors()) |>\n",
    "                  step_center(all_predictors())\n",
    "\n",
    "# k-nn Classification spec\n",
    "knn_spec_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "              set_engine(\"kknn\") |>\n",
    "              set_mode(\"classification\")\n",
    "\n",
    "# Setting possible values of best k to be in between 1 and 50\n",
    "gridvals <- tibble(neighbors = seq(1, 50))\n",
    "\n",
    "# Combining everything into a workflow and collecting the metrics to identify the best value of k\n",
    "knn_results <- workflow() |>\n",
    "                  add_recipe(heart_recipe) |>\n",
    "                  add_model(knn_spec_tune) |>\n",
    "                  tune_grid(resamples = heart_vfold, grid = gridvals) |>\n",
    "                  collect_metrics()\n",
    "\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb6605-a175-4f45-9112-55a1b3531737",
   "metadata": {},
   "source": [
    "Table 5. _Mean metrics and standard errors for different values of k._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a7e1d-8793-4149-926f-52b29ba0fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies <- knn_results |>\n",
    "  filter(.metric == \"accuracy\")\n",
    "\n",
    "# Creating a cross-validation plot to visualise accuracies for the different values of K\n",
    "cross_val_plot <- accuracies |> \n",
    "                    ggplot(aes(x = neighbors, y = mean)) +\n",
    "                    geom_point() +\n",
    "                    geom_line() +\n",
    "                    labs(x = \"Neighbors\", y = \"Accuracy Estimate\") +\n",
    "                    ggtitle(\"Graph of Accuracies for different k Values\") +\n",
    "                    theme(text = element_text(size = 15))\n",
    "\n",
    "cross_val_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddde2e-3ea8-44b1-a7a3-4aedc6c1c00e",
   "metadata": {},
   "source": [
    "Figure 2. _Graph of Accuracy Estimate vs. Neighbors._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1457f9f-c08c-4081-966d-bd45f0720f7e",
   "metadata": {},
   "source": [
    "Upon graphing the accuracies of our classifier for different values of k, we can see that the best value of k is somewhere between 20 and 30. Now, we can sort the accuracy results in descending order of their mean accuracy, and then pick the first entry of the tibble, which is our best k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e606d810-92c7-49da-a0c7-6fc0f23fa158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the best value of K\n",
    "conclusions <- knn_results |>\n",
    "                filter(.metric == \"accuracy\") |>\n",
    "                select(neighbors, mean, std_err) |>\n",
    "                arrange(mean) |>\n",
    "                tail(1)\n",
    "\n",
    "conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e392f8e-bdbc-48b1-ad56-f451530460f5",
   "metadata": {},
   "source": [
    "Table 6. _Best value of k, accuracy and standard error._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44151275-43e0-496b-a593-e8c0dea7ebee",
   "metadata": {},
   "source": [
    "From the above conclusions, we can see that the best value for **k** to be used is **25**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc5545d-35da-467f-af95-9fbd097a4928",
   "metadata": {},
   "source": [
    "Now that we have identified the best value for k, we can make a new model specification for the best value of k and retrain the classifier. We will use the k-nearest neighbours to create a model specification with k = 25 and combine the model specification into a workflow that includes the recipe we previously made. Following, we will use the fit function to build the classifier. We will do so using the training data as we want to keep the testing data for the following steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5b1a8-a1e2-4fee-a7cd-a4de88e53373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining the classifer \n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 25) |>\n",
    "    set_engine(\"kknn\") |>\n",
    "    set_mode(\"classification\")\n",
    "\n",
    "knn_fit <- workflow() |>\n",
    "    add_recipe(heart_recipe) |>\n",
    "    add_model(knn_spec) |>\n",
    "    fit(data = heart_training)\n",
    "knn_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb955b-481f-42e4-ab6b-53c70441bf72",
   "metadata": {},
   "source": [
    "After retraining the classifier, we can now test it by using it to predict labels on our test set. We will use the predict function on our testing set and the bind_cols function to merge the column of our predictions to the original data set. Therefore, we will be able to make a direct comparison of the predicted labels to the actual labels, with \".pred_class\" representing our predictions and \"hd\" representing the true diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f91f2a-1dd3-4b8f-aea9-60aaa0d419d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the labels in the test set\n",
    "heart_test_predictions <- predict(knn_fit, heart_testing) |>\n",
    "    bind_cols(heart_testing)\n",
    "heart_test_predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ffbaaa-d4eb-43b5-b6f1-3499e29f4492",
   "metadata": {},
   "source": [
    "Table 7. _Classification of Heart Disease on the Test Set by classifier._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89b3a6-c5a2-4238-b937-c4e4a1f94f13",
   "metadata": {},
   "source": [
    "After predicting the labels on the testing set, we can follow our analysis by assessing our classifier's accuracy. In order to determine the quality of our model that we trained, we can use the metrics functions from tidymodels. We specify the column containing the true results in the truth argument and we use the estimate argument to denote which column contains our predictions. Further, we use the filter function to view the accuracy rows only. This step will help us analyse the performance of our classifier and it will give us an estimated accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4041d-28f2-4579-bca6-b2780cc98d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the accuracy\n",
    "heart_accuracies <- heart_test_predictions |>\n",
    "    metrics(truth = hd, estimate = .pred_class) |>\n",
    "    filter(.metric ==\"accuracy\")\n",
    "heart_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb49df5-9ec1-4da4-897d-e6382993242f",
   "metadata": {},
   "source": [
    "Table 8. _Final Accuracy for the Classifier._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d61223b-b1af-4bcd-afb9-837c4009b00b",
   "metadata": {},
   "source": [
    "The final step in our classification is to look at the confusion matrix for the classifier. This is a table that will compare the predicted labels against the correct labels. We can do so by using the conf_mat function and again, specifying which columns contain our predictions and also which column contains the true values. By conducting this final step, we can get a numerical representation of how many labels our classifier was able to successfully predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de727b-5d08-48ac-88a0-691dd16e47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the confusion matrix\n",
    "confusion <- heart_test_predictions |>\n",
    "             conf_mat(truth = hd, estimate = .pred_class)\n",
    "confusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1e351-83bb-4448-a0d4-ae32a937aaa0",
   "metadata": {},
   "source": [
    "Table 9. _Confusion Matrix for the classifier, comparing true positives, false positives, true negatives and false negatives._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec85390-87db-450d-95f5-bd319d619584",
   "metadata": {},
   "source": [
    "After looking at the confusion matrix for the classifier, we can see that 44 observations were correctly predicted as \"yes\" and 78 observations were correctly predicted as \"no\". However, our classifier did make some mistakes as it misclassified 9 observations as \"yes\" and 17 observations as \"no\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15559b5-186f-4d6b-99a2-366eeac2d7dc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc74ce7a-7f9d-49cc-95ea-03cb8b4b3291",
   "metadata": {},
   "source": [
    "**Discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f0570-993d-4f64-b753-960fbd7a2eed",
   "metadata": {},
   "source": [
    "Heart disease is the leading cause of death world wide and finding better ways to predict heart disease early is crucial in preventing deaths <a href=\"https://doi.org/10.1016/j.tele.2018.11.007\">(Amin et al., 2019)</a>. Heart disease is known as a “silent killer,” as 45% of all heart attacks do not display the typical symptoms of a heart attack <a href=\"https://www.health.harvard.edu/heart-health/the-danger-of-silent-heart-attacks\">(“Danger of ‘silent’,” 2020)</a>. <a href=\"https://doi.org/10.7326/0003-4819-157-7-201210020-00514\">MD Virginia A. Moyer (2012)</a> warns, the first symptom of heart disease for some people may be a sudden fatal heart attack. Thus, it is imperative to find a way to predict heart disease based on other health factors. Our classifier obtained an 82.43% accuracy on our test data set. This is a significant improvement upon our majority classifier, which would return a 59% accuracy. Therefore, our method is gaining valuable information from our predictors. Thus, by knowing \"oldpeak,\" \"cp,\" and \"thal\" can be used to predict heart disease with a 82.43% accuracy, a physician can take care to observe those factors in their patients leading to earlier detection. However, in practice an 82.43% accuracy should be improved upon because of the need for a reliable diagnosis of heart disease, due to the seriousness of the disease, sometimes lack of symptoms, and need for early intervention. Our classifier predicted “no” when in truth it was “yes” (a false negative diagnosis) almost twice the amount of times it predicted a false positive diagnosis of heart disease. Thus, there is room for improvement within our method since our classifier is not accurate 17.57% of the time and it may have fatal consequences. \n",
    "\n",
    "Using the Cleveland data set, <a href=\"https://doi.org/10.1016/j.tele.2018.11.007\">Amin et al. (2019)</a> ran seven classification techniques and determined the best accuracies. Using best subset selection and a binary classification, they found the highest accuracy when using k-nn classification to be 82.49% when using the predictors: sex, cp, fbs, restecg, oldpeak, ca, thal. This score is very similar to ours and using best subset selection appears to have only improved the accuracy score slightly. However, they found the best accuracy at 86.87% by using the Support Vector Machine technique and the predictors: age, sex, cp, chol, fbs, exang, oldpeak, slope, ca. Therefore to improve our accuracy score for real world applications we can consider using SVM analysis and choosing our predictors with the best subset selection method, which we’ve mentioned tries all possible combinations of predictors resulting in a higher accuracy than forward selection.\n",
    "\n",
    "In this project, we explored the factors that impact heart disease. We ran many tests to find the factors that will have the greatest influence in predicting whether or not a new patient will have heart disease. After concluding that rest (oldpeak), chest pain type (cp) and presence of heart defect (thal) were the best predictors, we noted a 82.43% accuracy on our test set. These conclusions are very encouraging as classifiers similar to the one we created can be used to predict not only heart disease but many other medical conditions and diseases. Even though our classifier had a 82.43% accuracy, the results can be used to identify the leading risk factors for heart disease. With that, if a medical professional notices a patient with a certain chest pain type, they may be more likely to investigate possible heart disease.\n",
    "\n",
    "Our findings from this project can lead us to future questions such as how we can use these types of classifiers to help predict other medical conditions. In the future, it is possible that classification can potentially help prevent the development of diseases as it may speed up the diagnostic process.\n",
    "\n",
    "In conclusion, our classifier has answered the question that we had set out to answer. Heart Disease can indeed be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbff436-d57c-47c4-bfab-f06c7ed9b18f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc4cb0-7b75-4230-9016-ff5cac71518b",
   "metadata": {},
   "source": [
    "**References**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab025c-c6f5-4c06-9905-5699f19078e0",
   "metadata": {},
   "source": [
    "Amin, M. S., Chiam, Y. K., & Varathan, K. D. (2019). Identification of significant features and data mining techniques in predicting heart disease. Telematics and Informatics, 36, 82-93. https://doi.org/10.1016/j.tele.2018.11.007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25bbe7-510f-4b26-b8ec-63602ab390c1",
   "metadata": {},
   "source": [
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e180d7-684f-4b8c-8d7d-a2f63218afde",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hawkins, D. M. (2004). The problem of overfitting. Journal of Chemical Information and Computer Sciences, 44(1), 1-12. https://doi.org/10.1021/ci0342472"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce10f14-af0d-43da-ab3f-cbd9ce65f9d2",
   "metadata": {},
   "source": [
    "Heart Disease in USA gif: https://lh5.googleusercontent.com/hxLpYW3mP53JJSnWvUL9bB8zWvezzLWVHklVYPm2owl3xyik3PtPibwK5lXMy98fKICJZK7Ee8EP7ZvIUTVCey4ZW0_SftMKeMeooft9MoRiYOTdO2S2duo6QgiArcqZ9Y8qfe-E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1b4f0-c6ce-4cef-b44b-82b8afd7821b",
   "metadata": {},
   "source": [
    "Heart gif: https://gifs.eco.br/wp-content/uploads/2022/08/gifs-do-sistema-cardiovascular-0.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df874898-4d12-4517-b667-1a2befd60779",
   "metadata": {},
   "source": [
    "Johns Hopkins Medicine (2017, January 11). _Updated Classification System Captures Many More People at Risk for Heart Attack_. https://www.hopkinsmedicine.org/news/media/releases/updated_classification_system_captures_many_more_people_at_risk_for_heart_attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea2907-f0d7-414a-8ef4-78ba4e9703f5",
   "metadata": {},
   "source": [
    "MOYER, V. A., & U.S. Preventive Services Task Force. (2012). Screening for coronary heart disease with electrocardiography: U.S. preventive services task force recommendation statement. Annals of Internal Medicine, 157(7), 512-518. https://doi.org/10.7326/0003-4819-157-7-201210020-00514"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30efde17-a398-4f6f-b5aa-96db1bddd636",
   "metadata": {},
   "source": [
    "Rocha, A. D., Groen, T. A., Skidmore, A. K., Darvishzadeh, R., & Willemen, L. (2017). The naïve overfitting index selection (NOIS): A new method to optimize model complexity for hyperspectral data. ISPRS Journal of Photogrammetry and Remote Sensing, 133, 61-74. https://doi.org/10.1016/j.isprsjprs.2017.09.012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f725982-73dd-4688-ab94-7ca210f8c495",
   "metadata": {},
   "source": [
    "The danger of “silent” heart attacks. (2020, November 3). Harvard Health Publishing. https://www.health.harvard.edu/heart-health/the-danger-of-silent-heart-attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6c508-3b74-43a9-9a63-26f56c23bf05",
   "metadata": {},
   "source": [
    "Timbers, T., Campbell, T., & Lee, M. (2022). Data science: A first introduction. CRC Press LLC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
